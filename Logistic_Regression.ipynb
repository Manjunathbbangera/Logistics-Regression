{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY QUESTIONS\n"
      ],
      "metadata": {
        "id": "-gBKnQ8D7jok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-  What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "Ans- Logistic Regression:\n",
        "Used for classification; predicts probability using a sigmoid function.\n",
        "\n",
        "Linear Regression:\n",
        "Used for regression; predicts continuous values using a linear equation.\n",
        "\n",
        "Difference:\n",
        "Logistic Regression â†’ Classification\n",
        "Linear Regression â†’ Prediction of continuous values.\n",
        "\n",
        "\n",
        "2- What is the mathematical equation of Logistic Regression ?\n",
        "Ans- The **mathematical equation** of Logistic Regression is:\n",
        "\n",
        "$$\n",
        "P(y = 1 | x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $P(y = 1 | x)$ is the probability of class 1,\n",
        "* $\\beta_0$ is the intercept,\n",
        "* $\\beta_1, \\beta_2, \\dots, \\beta_n$ are the coefficients,\n",
        "* $x_1, x_2, \\dots, x_n$ are the input features.\n",
        "\n",
        "3- Why do we use the Sigmoid function in Logistic Regression ?\n",
        "\n",
        "Ans- We use the **sigmoid function** in **Logistic Regression** because:\n",
        "\n",
        "### ðŸ”¹ It maps any real-valued number to a value between **0 and 1**,\n",
        "\n",
        "which is perfect for representing **probabilities**.\n",
        "\n",
        "### ðŸ”¹ The sigmoid function is:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "This helps us:\n",
        "\n",
        "* Interpret the output as a **probability**,\n",
        "* Classify data into **binary categories** (e.g., spam or not spam).\n",
        "\n",
        "In short:\n",
        "ðŸ‘‰ **Sigmoid turns linear output into a probability for classification.**\n",
        "\n",
        "\n",
        "\n",
        "4- What is the cost function of Logistic Regression ?\n",
        "\n",
        "Ans- The **cost function** of Logistic Regression is the **Log Loss** or **Binary Cross-Entropy**.\n",
        "\n",
        "### ðŸ”¹ **Formula:**\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $m$ = number of training examples\n",
        "* $y^{(i)}$ = actual label (0 or 1)\n",
        "* $h_\\theta(x^{(i)})$ = predicted probability (sigmoid output)\n",
        "\n",
        "### ðŸ”¹ Purpose:\n",
        "\n",
        "It measures how well the model's predicted probabilities match the actual labels.\n",
        "ðŸ‘‰ **Lower cost = better prediction accuracy.**\n",
        "\n",
        "\n",
        "5- What is Regularization in Logistic Regression? Why is it needed ?\n",
        "\n",
        "Ans- ### ðŸ”¹ **What is Regularization in Logistic Regression?**\n",
        "\n",
        "Regularization is a technique used to **prevent overfitting** by adding a penalty term to the cost function.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ **Why is it needed?**\n",
        "\n",
        "* To **reduce model complexity**\n",
        "* To **avoid overfitting** on training data\n",
        "* Helps the model **generalize better** on unseen data\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ **Types of Regularization:**\n",
        "\n",
        "1. **L1 Regularization (Lasso):** Adds sum of absolute values of coefficients\n",
        "\n",
        "   $$\n",
        "   \\text{Penalty: } \\lambda \\sum |\\beta_j|\n",
        "   $$\n",
        "\n",
        "2. **L2 Regularization (Ridge):** Adds sum of squared coefficients\n",
        "\n",
        "   $$\n",
        "   \\text{Penalty: } \\lambda \\sum \\beta_j^2\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ In short:\n",
        "\n",
        "**Regularization** keeps the model **simple and accurate** by penalizing large weights.\n",
        "\n",
        "\n",
        "6- Explain the difference between Lasso, Ridge, and Elastic Net regression ?\n",
        "\n",
        "Ans- Hereâ€™s a **short and clear comparison** of **Lasso**, **Ridge**, and **Elastic Net** regression:\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ **Ridge Regression (L2 Regularization)**\n",
        "\n",
        "* **Penalty:** Sum of **squares** of coefficients\n",
        "\n",
        "  $$\n",
        "  \\lambda \\sum \\beta_j^2\n",
        "  $$\n",
        "* **Effect:** Shrinks coefficients but **does not set them to zero**\n",
        "* **Use when:** You want to keep all features, just reduce their impact\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ **Lasso Regression (L1 Regularization)**\n",
        "\n",
        "* **Penalty:** Sum of **absolute values** of coefficients\n",
        "\n",
        "  $$\n",
        "  \\lambda \\sum |\\beta_j|\n",
        "  $$\n",
        "* **Effect:** Shrinks some coefficients to **exactly zero** (feature selection)\n",
        "* **Use when:** You want a **sparse model** (only important features)\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ **Elastic Net Regression**\n",
        "\n",
        "* **Penalty:** Combines L1 and L2\n",
        "\n",
        "  $$\n",
        "  \\lambda_1 \\sum |\\beta_j| + \\lambda_2 \\sum \\beta_j^2\n",
        "  $$\n",
        "* **Effect:** Balance between **Ridge and Lasso**\n",
        "* **Use when:** Features are **correlated** or you want **both regularization and feature selection**\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… **Summary Table:**\n",
        "\n",
        "| Method          | Penalty Type  | Sets Coefficients to 0 | Use Case                              |\n",
        "| --------------- | ------------- | ---------------------- | ------------------------------------- |\n",
        "| **Ridge**       | L2 (squared)  | âŒ No                   | Keep all features, reduce overfitting |\n",
        "| **Lasso**       | L1 (absolute) | âœ… Yes                  | Feature selection (sparse model)      |\n",
        "| **Elastic Net** | L1 + L2       | âœ… Sometimes            | Mixed data, correlated features       |\n",
        "\n",
        "\n",
        "7- When should we use Elastic Net instead of Lasso or Ridge.\n",
        "\n",
        "Ans- ### âœ… **Use Elastic Net when:**\n",
        "\n",
        "1. **Features are highly correlated**\n",
        "\n",
        "   * Lasso may randomly pick one and ignore others.\n",
        "   * Elastic Net can keep **groups of related features**.\n",
        "\n",
        "2. **You want both:**\n",
        "\n",
        "   * **Feature selection** (like Lasso), and\n",
        "   * **Model stability** (like Ridge)\n",
        "\n",
        "3. **Lasso fails (too few features selected)**\n",
        "\n",
        "   * When Lasso removes **too many features**, Elastic Net balances it.\n",
        "\n",
        "4. **Number of features > Number of samples**\n",
        "\n",
        "   * Elastic Net handles **high-dimensional data** better.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ In short:\n",
        "\n",
        "Use **Elastic Net** when you need **feature selection + robustness** in models with **many or correlated features**.\n",
        "\n",
        "8- What is the impact of the regularization parameter (Î») in Logistic Regression ?\n",
        "\n",
        "Ans- ### ðŸ”¹ **Impact of Regularization Parameter (Î») in Logistic Regression:**\n",
        "\n",
        "* **Î» (lambda)** controls the **strength of regularization**.\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… **Effects:**\n",
        "\n",
        "| Î» Value     | Effect on Model                                                                           |\n",
        "| ----------- | ----------------------------------------------------------------------------------------- |\n",
        "| **Î» = 0**   | No regularization â†’ may **overfit**                                                       |\n",
        "| **Small Î»** | Light regularization â†’ slight penalty on weights                                          |\n",
        "| **Large Î»** | Strong regularization â†’ **simpler model**, may **underfit** by shrinking weights too much |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ In short:\n",
        "\n",
        "* **Higher Î»** â†’ simpler model, less overfitting, risk of underfitting\n",
        "* **Lower Î»** â†’ complex model, better fit to training data, risk of overfitting\n",
        "\n",
        "**Goal:** Find the **best Î»** using techniques like **cross-validation**.\n",
        "\n",
        "9- What are the key assumptions of Logistic Regression ?\n",
        "\n",
        "Ans- ### **Key Assumptions of Logistic Regression:**\n",
        "\n",
        "1. **Linear relationship between predictors and log-odds**\n",
        "\n",
        "   * Assumes a **linear relationship** between input variables and the log-odds of the outcome.\n",
        "\n",
        "2. **Independence of observations**\n",
        "\n",
        "   * Each observation should be **independent** of the others (no autocorrelation).\n",
        "\n",
        "3. **Binary outcome**\n",
        "\n",
        "   * Logistic regression is typically used for **binary classification** (0 or 1).\n",
        "\n",
        "4. **No multicollinearity**\n",
        "\n",
        "   * The independent variables should not be **highly correlated** with each other.\n",
        "\n",
        "5. **Large sample size**\n",
        "\n",
        "   * A larger dataset is preferred, as small datasets may lead to unstable estimates.\n",
        "\n",
        "6. **Linearity of independent variables**\n",
        "\n",
        "   * The model assumes that the relationship between the **predictors and the log-odds** of the outcome is linear.\n",
        "\n",
        "---\n",
        "\n",
        "In short, **Logistic Regression** works best when the relationship between the inputs and the output is **log-linear**, and data is **independent** and **not highly collinear**.\n",
        "\n",
        "\n",
        "10- What are some alternatives to Logistic Regression for classification tasks ?\n",
        "\n",
        "Ans- Here are some common **alternatives to Logistic Regression** for classification tasks:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Decision Trees**\n",
        "\n",
        "* **Description:** A tree-like structure where decisions are made based on feature values.\n",
        "* **Advantages:** Easy to interpret, handles both numerical and categorical data.\n",
        "* **Disadvantages:** Prone to overfitting if not pruned.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Random Forest**\n",
        "\n",
        "* **Description:** An ensemble method that builds multiple decision trees and averages their predictions.\n",
        "* **Advantages:** Reduces overfitting, handles large datasets well.\n",
        "* **Disadvantages:** Less interpretable, more computationally expensive.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Support Vector Machines (SVM)**\n",
        "\n",
        "* **Description:** Finds the hyperplane that best separates different classes in high-dimensional space.\n",
        "* **Advantages:** Works well for high-dimensional data, effective in complex classification problems.\n",
        "* **Disadvantages:** Computationally expensive, harder to interpret.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **K-Nearest Neighbors (KNN)**\n",
        "\n",
        "* **Description:** Classifies a data point based on the majority class of its nearest neighbors.\n",
        "* **Advantages:** Simple and intuitive, no model training required.\n",
        "* **Disadvantages:** Slow for large datasets, sensitive to irrelevant features.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Naive Bayes**\n",
        "\n",
        "* **Description:** Based on Bayes' Theorem, it assumes features are conditionally independent.\n",
        "* **Advantages:** Fast, works well with small datasets, and handles categorical data.\n",
        "* **Disadvantages:** Assumption of feature independence may not hold in practice.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Gradient Boosting Machines (GBM) / XGBoost / LightGBM**\n",
        "\n",
        "* **Description:** An ensemble method that builds trees sequentially to correct errors of previous trees.\n",
        "* **Advantages:** Often provides excellent predictive performance.\n",
        "* **Disadvantages:** More complex to tune, computationally expensive.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Neural Networks (Deep Learning)**\n",
        "\n",
        "* **Description:** Uses multiple layers of neurons to learn complex patterns in data.\n",
        "* **Advantages:** Can model highly complex relationships, performs well on large datasets.\n",
        "* **Disadvantages:** Requires large amounts of data, computationally expensive, harder to interpret.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "\n",
        "* **Simple & Interpretable:** Decision Trees, Naive Bayes\n",
        "* **Complex & High Performance:** Random Forest, XGBoost, Neural Networks\n",
        "* **Good for High-Dimensional Data:** SVM, Neural Networks\n",
        "\n",
        "Each method has its strengths and weaknesses, so the choice depends on the dataset, problem complexity, and desired performance.\n",
        "\n",
        "\n",
        "11- What are Classification Evaluation Metrics ?\n",
        "\n",
        "Ans- ### **Classification Evaluation Metrics:**\n",
        "\n",
        "These metrics are used to evaluate the performance of classification models. Here are the key ones:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Accuracy**\n",
        "\n",
        "* **Definition:** The proportion of correct predictions to the total predictions.\n",
        "* **Formula:**\n",
        "\n",
        "  $$\n",
        "  \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Population}}\n",
        "  $$\n",
        "* **Use:** Good when the class distribution is balanced.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Precision**\n",
        "\n",
        "* **Definition:** The proportion of positive predictions that are actually correct.\n",
        "* **Formula:**\n",
        "\n",
        "  $$\n",
        "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
        "  $$\n",
        "* **Use:** Important when **false positives** are costly (e.g., in spam detection).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "* **Definition:** The proportion of actual positives that are correctly identified by the model.\n",
        "* **Formula:**\n",
        "\n",
        "  $$\n",
        "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
        "  $$\n",
        "* **Use:** Important when **false negatives** are costly (e.g., in medical diagnoses).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **F1-Score**\n",
        "\n",
        "* **Definition:** The harmonic mean of Precision and Recall. Balances Precision and Recall.\n",
        "* **Formula:**\n",
        "\n",
        "  $$\n",
        "  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "  $$\n",
        "* **Use:** Useful when you need a balance between Precision and Recall.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Specificity (True Negative Rate)**\n",
        "\n",
        "* **Definition:** The proportion of actual negatives that are correctly identified by the model.\n",
        "* **Formula:**\n",
        "\n",
        "  $$\n",
        "  \\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}}\n",
        "  $$\n",
        "* **Use:** Important when **false positives** should be minimized.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**\n",
        "\n",
        "* **Definition:** Measures the ability of the model to discriminate between positive and negative classes. AUC represents the area under the ROC curve.\n",
        "* **Use:** A good metric when you want to compare models or need a threshold-independent measure.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Confusion Matrix**\n",
        "\n",
        "* **Definition:** A matrix that shows the true positives, true negatives, false positives, and false negatives.\n",
        "* **Use:** Provides a detailed breakdown of the modelâ€™s performance.\n",
        "\n",
        "|                     | Predicted Positive  | Predicted Negative  |\n",
        "| ------------------- | ------------------- | ------------------- |\n",
        "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
        "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary:**\n",
        "\n",
        "* **Accuracy:** Good for balanced datasets.\n",
        "* **Precision & Recall:** Useful when classes are imbalanced or when certain types of errors (false positives or false negatives) matter more.\n",
        "* **F1-Score:** Balances precision and recall, especially when the dataset is imbalanced.\n",
        "* **AUC-ROC:** Useful for binary classification with different thresholds.\n",
        "\n",
        "Each metric helps to highlight different aspects of model performance, depending on your specific problem and data!\n",
        "\n",
        "\n",
        "12- How does class imbalance affect Logistic Regression ?\n",
        "\n",
        "Ans- Hyperparameter tuning in logistic regression involves adjusting parameters like regularization strength, solver, penalty type, and iteration limit to improve model performance. Using techniques like Grid Search or Random Search with Cross-Validation can help you find the optimal settings for your model\n",
        "\n",
        "\n",
        "13- What is Hyperparameter Tuning in Logistic Regression.\n",
        "\n",
        "Ans- ### **Hyperparameter Tuning in Logistic Regression**\n",
        "\n",
        "**Hyperparameter tuning** refers to the process of selecting the best set of hyperparameters for a model, in this case, **Logistic Regression**. Hyperparameters are values set before training the model, and their optimization can significantly improve the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Hyperparameters in Logistic Regression:**\n",
        "\n",
        "1. **Regularization Strength (C or Î»)**\n",
        "\n",
        "   * **Description:** Controls the strength of regularization to avoid overfitting.\n",
        "\n",
        "     * **C:** Inverse of regularization strength (higher C means weaker regularization).\n",
        "     * **Î»:** Regularization strength (higher Î» means stronger regularization).\n",
        "   * **Effect:**\n",
        "\n",
        "     * **High C (Low Î»)** â†’ model fits the data better but risks overfitting.\n",
        "     * **Low C (High Î»)** â†’ model becomes simpler, reducing overfitting but might underfit.\n",
        "\n",
        "2. **Penalty (L1 vs. L2)**\n",
        "\n",
        "   * **Description:** The type of regularization used.\n",
        "\n",
        "     * **L1:** Lasso regularization (feature selection, sparse models).\n",
        "     * **L2:** Ridge regularization (prevents large coefficients but doesn't set them to zero).\n",
        "   * **Effect:**\n",
        "\n",
        "     * **L1:** Useful for sparse models (feature selection).\n",
        "     * **L2:** More commonly used for general regularization.\n",
        "\n",
        "3. **Solver**\n",
        "\n",
        "   * **Description:** The algorithm used to optimize the cost function.\n",
        "\n",
        "     * Common solvers: **'liblinear', 'newton-cg', 'lbfgs', 'saga'**.\n",
        "   * **Effect:** The choice of solver affects the speed and convergence of the optimization process.\n",
        "\n",
        "     * **'liblinear'** is good for smaller datasets.\n",
        "     * **'newton-cg', 'lbfgs'** work well for large datasets.\n",
        "     * **'saga'** is best for very large and sparse datasets.\n",
        "\n",
        "4. **Maximum Iterations (max\\_iter)**\n",
        "\n",
        "   * **Description:** The maximum number of iterations the solver can take to converge.\n",
        "   * **Effect:** Increasing the number of iterations can help the model converge better, especially for complex datasets.\n",
        "\n",
        "5. **Tolerance (tol)**\n",
        "\n",
        "   * **Description:** The stopping criterion. Determines when to stop the optimization process.\n",
        "   * **Effect:** A smaller tolerance value means more precision and more computation time, as the model continues to optimize.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Tune Hyperparameters?**\n",
        "\n",
        "* **Grid Search**: Search exhaustively through a manually specified set of hyperparameters.\n",
        "\n",
        "  * **Pros**: Guarantees that the best set of hyperparameters is found within the specified grid.\n",
        "  * **Cons**: Computationally expensive.\n",
        "\n",
        "* **Random Search**: Randomly samples hyperparameter combinations.\n",
        "\n",
        "  * **Pros**: Faster and can be more efficient than grid search, especially for large hyperparameter spaces.\n",
        "  * **Cons**: No guarantee of finding the best combination.\n",
        "\n",
        "* **Cross-Validation**: Used alongside Grid or Random Search to ensure the model performs well on unseen data and avoids overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary:**\n",
        "\n",
        "* **Hyperparameter tuning** improves Logistic Regression performance by adjusting values like regularization strength (C), penalty type (L1 or L2), solver, and iteration count.\n",
        "* **Grid Search** and **Random Search** are common methods for finding the best hyperparameters, usually combined with **cross-validation** to evaluate performance.\n",
        "\n",
        "\n",
        "14- What are different solvers in Logistic Regression? Which one should be used ?\n",
        "\n",
        "Ans- In **Logistic Regression**, the **solver** refers to the algorithm used to optimize the modelâ€™s cost function and find the best-fitting parameters (coefficients). Different solvers are available, and the choice of solver depends on factors like dataset size, regularization type, and computational efficiency.\n",
        "\n",
        "### **Common Solvers in Logistic Regression:**\n",
        "\n",
        "1. **'liblinear'**\n",
        "\n",
        "   * **Description:** Uses **Coordinate Descent** for optimization. Works well for small datasets and when using **L1 regularization (Lasso)**.\n",
        "   * **Use Case:** Small datasets, L1 regularization, binary classification.\n",
        "   * **Advantages:** Handles **L1 regularization** well, efficient for smaller datasets.\n",
        "   * **Disadvantages:** May not scale well for larger datasets.\n",
        "\n",
        "2. **'newton-cg'**\n",
        "\n",
        "   * **Description:** Uses **Newton's method** (second-order optimization) for optimization. Suitable for larger datasets.\n",
        "   * **Use Case:** Large datasets, L2 regularization (Ridge).\n",
        "   * **Advantages:** Fast convergence on large datasets, handles **L2 regularization** efficiently.\n",
        "   * **Disadvantages:** More computationally intensive than other methods.\n",
        "\n",
        "3. **'lbfgs'**\n",
        "\n",
        "   * **Description:** Uses **Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)**, a quasi-Newton method. Suitable for large datasets and L2 regularization.\n",
        "   * **Use Case:** Large datasets, L2 regularization.\n",
        "   * **Advantages:** Efficient for large datasets, handles **L2 regularization**.\n",
        "   * **Disadvantages:** Might be slower than â€˜sagaâ€™ on very large datasets.\n",
        "\n",
        "4. **'saga'**\n",
        "\n",
        "   * **Description:** Uses **Stochastic Average Gradient** descent, a variant of **stochastic gradient descent** (SGD). It works for both **L1** and **L2 regularization** and is very efficient on large datasets.\n",
        "   * **Use Case:** Large datasets, L1 and L2 regularization, sparse datasets.\n",
        "   * **Advantages:** Supports both **L1 and L2 regularization**, fast for large, sparse datasets.\n",
        "   * **Disadvantages:** More computationally expensive for smaller datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Which Solver Should Be Used?**\n",
        "\n",
        "* **For small datasets**:\n",
        "  Use **'liblinear'** if you're working with smaller datasets or using **L1 regularization** (Lasso).\n",
        "\n",
        "* **For larger datasets**:\n",
        "  Use **'newton-cg'**, **'lbfgs'**, or **'saga'**. These solvers are more efficient for handling large datasets.\n",
        "\n",
        "  * **'newton-cg'** and **'lbfgs'** are suitable for **L2 regularization**.\n",
        "  * **'saga'** is useful for both **L1 and L2 regularization** and is the best option for very large, sparse datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Solver Choice:**\n",
        "\n",
        "| Solver          | Regularization | Dataset Size | Use Case                                    |\n",
        "| --------------- | -------------- | ------------ | ------------------------------------------- |\n",
        "| **'liblinear'** | L1, L2         | Small        | Small datasets, binary classification       |\n",
        "| **'newton-cg'** | L2             | Large        | Large datasets, L2 regularization           |\n",
        "| **'lbfgs'**     | L2             | Large        | Large datasets, L2 regularization           |\n",
        "| **'saga'**      | L1, L2         | Large        | Large/sparse datasets, both regularizations |\n",
        "\n",
        "Choosing the right solver based on your dataset and regularization choice ensures the best performance and efficiency for your logistic regression model.\n",
        "\n",
        "\n",
        "15- How is Logistic Regression extended for multiclass classification >\n",
        "\n",
        "Ans- ### **Extending Logistic Regression for Multiclass Classification**\n",
        "\n",
        "Logistic Regression is inherently a **binary classifier**, meaning it can predict only two classes (0 or 1). However, it can be extended to handle **multiclass classification** (i.e., more than two classes) using the following strategies:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **One-vs-Rest (OvR) or One-vs-All (OvA)**\n",
        "\n",
        "* **Approach:** For each class, fit a separate binary classifier that predicts whether an instance belongs to that class or not.\n",
        "\n",
        "  * **For example**, if you have three classes (A, B, C), you would train three binary classifiers:\n",
        "\n",
        "    * Classifier 1: Class A vs. Not Class A (Class B or Class C)\n",
        "    * Classifier 2: Class B vs. Not Class B (Class A or Class C)\n",
        "    * Classifier 3: Class C vs. Not Class C (Class A or Class B)\n",
        "* **Prediction:** During prediction, each classifier outputs a probability, and the class with the highest probability is chosen as the final predicted class.\n",
        "* **Use Case:** Useful when classes are independent.\n",
        "* **Disadvantages:** Can lead to **imbalanced performance** if one class is much more frequent than others.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **One-vs-One (OvO)**\n",
        "\n",
        "* **Approach:** A separate binary classifier is trained for every pair of classes. If you have `n` classes, you will train $\\binom{n}{2}$ classifiers.\n",
        "\n",
        "  * **For example**, if you have three classes (A, B, C), you would train:\n",
        "\n",
        "    * Classifier 1: Class A vs. Class B\n",
        "    * Classifier 2: Class A vs. Class C\n",
        "    * Classifier 3: Class B vs. Class C\n",
        "* **Prediction:** During prediction, each classifier votes for one class, and the class with the most votes is chosen.\n",
        "* **Use Case:** Useful when classes are well-separated.\n",
        "* **Disadvantages:** Computationally expensive, as the number of classifiers grows quadratically with the number of classes.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Softmax Regression (Multinomial Logistic Regression)**\n",
        "\n",
        "* **Approach:** Instead of using multiple binary classifiers, **Softmax Regression** extends logistic regression directly to multiclass problems by predicting the probabilities of multiple classes simultaneously.\n",
        "* **Mathematical Formula:**\n",
        "  For $k$ classes, the model computes the probabilities of each class $P(y=k|x)$ using the **Softmax function**:\n",
        "\n",
        "  $$\n",
        "  P(y=k|x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}\n",
        "  $$\n",
        "\n",
        "  Where:\n",
        "\n",
        "  * $\\theta_k$ is the parameter vector for class $k$,\n",
        "  * $K$ is the number of classes,\n",
        "  * $x$ is the input feature vector.\n",
        "* **Prediction:** The model outputs the class with the highest probability.\n",
        "* **Use Case:** This is the **most common method** for multiclass classification with Logistic Regression.\n",
        "* **Advantages:** More efficient than One-vs-Rest or One-vs-One; provides direct probability estimates for each class.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison of Methods:**\n",
        "\n",
        "| Method          | Description                                   | Number of Classifiers   | Complexity                     |\n",
        "| --------------- | --------------------------------------------- | ----------------------- | ------------------------------ |\n",
        "| **One-vs-Rest** | One classifier per class, binary.             | $K$ (number of classes) | Low computation per classifier |\n",
        "| **One-vs-One**  | One classifier per pair of classes.           | $\\binom{K}{2}$          | High computational cost        |\n",
        "| **Softmax**     | A single classifier that handles all classes. | 1                       | Most efficient and scalable    |\n",
        "\n",
        "---\n",
        "\n",
        "### **Which Method to Choose?**\n",
        "\n",
        "* **Softmax Regression (Multinomial Logistic Regression)** is typically the best choice for **multiclass classification** since it is **computationally efficient** and avoids the complexity of multiple binary classifiers.\n",
        "* **One-vs-Rest** is simple to implement and often used when classes are not highly dependent.\n",
        "* **One-vs-One** can be used if you have **small numbers of classes** and require fine distinctions between pairs of classes, but it is generally **more computationally expensive**.\n",
        "\n",
        "In practice, **Softmax Regression** is most commonly used for multiclass logistic regression.\n",
        "\n",
        "\n",
        "16- What are the advantages and disadvantages of Logistic Regression ?\n",
        "\n",
        "Ans- ### **Advantages of Logistic Regression:**\n",
        "\n",
        "1. **Simplicity and Interpretability**\n",
        "\n",
        "   * Logistic Regression is easy to implement and understand. The model is linear, and its coefficients can be interpreted as the influence of each feature on the log-odds of the target variable.\n",
        "\n",
        "2. **Efficient and Fast**\n",
        "\n",
        "   * It is computationally efficient and works well on small to medium-sized datasets. It can also scale well with a large number of features when regularization is applied.\n",
        "\n",
        "3. **Probabilistic Outputs**\n",
        "\n",
        "   * Logistic Regression provides probability estimates for each class, which can be useful for decision-making, especially in situations where you want to know how confident the model is in its predictions.\n",
        "\n",
        "4. **Less Prone to Overfitting (with Regularization)**\n",
        "\n",
        "   * By applying **L1 (Lasso)** or **L2 (Ridge)** regularization, logistic regression can prevent overfitting, especially when the number of features is large compared to the number of data points.\n",
        "\n",
        "5. **Works Well with Linearly Separable Data**\n",
        "\n",
        "   * Logistic Regression performs well when the classes are linearly separable or nearly linearly separable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Logistic Regression:**\n",
        "\n",
        "1. **Assumption of Linearity**\n",
        "\n",
        "   * Logistic Regression assumes a **linear relationship** between the input variables and the log-odds of the target variable, which may not hold in more complex datasets. Non-linear relationships may lead to poor performance.\n",
        "\n",
        "2. **Struggles with Complex Data**\n",
        "\n",
        "   * It does not capture complex patterns, and thus may struggle with datasets that have complex, non-linear decision boundaries. Other models like **Decision Trees**, **Random Forests**, or **Neural Networks** might perform better in such cases.\n",
        "\n",
        "3. **Sensitive to Outliers**\n",
        "\n",
        "   * Logistic Regression can be sensitive to outliers, which may disproportionately influence the model. Data preprocessing (such as handling outliers) or using more robust models can help.\n",
        "\n",
        "4. **Limited to Binary and Multiclass with Extensions**\n",
        "\n",
        "   * While **binary classification** is straightforward, **multiclass classification** requires extensions like **One-vs-Rest (OvR)** or **Softmax Regression**, making it more complicated compared to methods inherently designed for multiclass tasks.\n",
        "\n",
        "5. **Poor Performance with Highly Correlated Features**\n",
        "\n",
        "   * Logistic Regression assumes that features are **independent** of each other. When there is high multicollinearity (correlation) between features, it can lead to unreliable estimates and high variance in the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **In Summary:**\n",
        "\n",
        "| **Advantages**                                  | **Disadvantages**                                |\n",
        "| ----------------------------------------------- | ------------------------------------------------ |\n",
        "| Simple and interpretable                        | Assumes linearity between features and output    |\n",
        "| Computationally efficient and fast              | Struggles with complex or non-linear data        |\n",
        "| Provides probabilistic predictions              | Sensitive to outliers                            |\n",
        "| Less prone to overfitting (with regularization) | Requires careful handling of correlated features |\n",
        "| Works well for linearly separable data          | Needs extensions for multiclass classification   |\n",
        "\n",
        "Logistic Regression is an excellent choice when the relationships in your data are simple and linearly separable, but it might not be the best option for more complex, non-linear datasets.\n",
        "\n",
        "\n",
        "17- What are some use cases of Logistic Regression ?\n",
        "\n",
        "Ans- Logistic Regression is highly versatile and can be applied across a wide range of domains like healthcare, marketing, finance, and social media. It is especially effective for binary and multiclass classification tasks, where the outcome is a categorical variable.\n",
        "\n",
        "\n",
        "18-  What is the difference between Softmax Regression and Logistic Regression ?\n",
        "\n",
        "Ans- ### **Difference Between Softmax Regression and Logistic Regression**\n",
        "\n",
        "Both **Logistic Regression** and **Softmax Regression** are used for classification tasks, but they differ in their scope and application, especially when dealing with multiclass problems. Below is a comparison of the two:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Scope of Application**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Primarily used for **binary classification** problems, where the target variable has two classes (e.g., 0 or 1, yes or no).\n",
        "  * It outputs a probability for the binary outcome.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Used for **multiclass classification** problems, where the target variable has more than two classes (e.g., multiple categories like dog, cat, bird, etc.).\n",
        "  * It outputs a probability distribution over multiple classes, ensuring that the sum of the probabilities is 1.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Model Output**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Outputs a **single probability** that an instance belongs to one of the two classes.\n",
        "  * The output is between 0 and 1, representing the probability of belonging to the positive class.\n",
        "  * The decision rule is typically: **Class = 1 if P(Class=1) > 0.5, else Class = 0**.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Outputs a **probability distribution** over multiple classes (for $K$ classes).\n",
        "  * The output is a vector of probabilities, one for each class. The sum of these probabilities equals 1.\n",
        "  * The decision rule is: **Choose the class with the highest probability**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Mathematical Formulation**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Uses the **sigmoid function** to model the probability of a binary outcome:\n",
        "\n",
        "    $$\n",
        "    P(y=1|X) = \\frac{1}{1 + e^{-(\\theta^T x)}}\n",
        "    $$\n",
        "  * Where $\\theta$ is the parameter vector, and $x$ is the feature vector.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Uses the **Softmax function** to model the probabilities for $K$ classes:\n",
        "\n",
        "    $$\n",
        "    P(y=k|X) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}\n",
        "    $$\n",
        "  * Where $\\theta_k$ is the parameter vector for class $k$, and $x$ is the feature vector.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Decision Rule**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * If the predicted probability $P(y=1|x)$ is greater than 0.5, the model predicts the positive class (Class 1), otherwise, it predicts the negative class (Class 0).\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Predict the class with the highest probability, i.e., the class $k$ with the maximum $P(y=k|x)$.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Usage of Regularization**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Can use **L1** (Lasso) or **L2** (Ridge) regularization to prevent overfitting.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Also uses **L1** or **L2** regularization, but typically applied to all the classes' parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Multiclass Capability**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Primarily for **binary classification**. For multiclass classification, extensions like **One-vs-Rest** (OvR) or **One-vs-One** (OvO) are required.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Natively designed for **multiclass classification**. It directly handles more than two classes without requiring additional strategies.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| **Feature**               | **Logistic Regression**                     | **Softmax Regression**                           |\n",
        "| ------------------------- | ------------------------------------------- | ------------------------------------------------ |\n",
        "| **Purpose**               | Binary classification                       | Multiclass classification                        |\n",
        "| **Output**                | Probability for one class (0 or 1)          | Probability distribution across multiple classes |\n",
        "| **Mathematical Function** | Sigmoid function                            | Softmax function                                 |\n",
        "| **Number of Classes**     | 2 classes (binary)                          | Multiple classes (more than 2)                   |\n",
        "| **Decision Rule**         | Class 1 if probability > 0.5, else Class 0  | Class with highest probability                   |\n",
        "| **Multiclass Handling**   | Requires One-vs-Rest or One-vs-One strategy | Directly handles multiclass classification       |\n",
        "| **Common Regularization** | L1 or L2 regularization                     | L1 or L2 regularization                          |\n",
        "\n",
        "---\n",
        "\n",
        "### **In Summary:**\n",
        "\n",
        "* **Logistic Regression** is suitable for binary classification, outputting a single probability for the positive class.\n",
        "* **Softmax Regression** is an extension of logistic regression for **multiclass classification**, outputting a probability distribution across multiple classes, and selecting the class with the highest probability.\n",
        "\n",
        "\n",
        "19- How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification ?\n",
        "\n",
        "Ans- ### **Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification**\n",
        "\n",
        "Both **One-vs-Rest (OvR)** and **Softmax** are commonly used techniques for handling **multiclass classification** problems. The choice between them depends on factors like the dataset size, model complexity, and specific problem requirements.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. One-vs-Rest (OvR)**\n",
        "\n",
        "In **One-vs-Rest**, you train a separate binary classifier for each class. Each classifier tries to distinguish one class from all the others. When making a prediction, the model outputs the class corresponding to the classifier that has the highest confidence.\n",
        "\n",
        "#### **When to use OvR:**\n",
        "\n",
        "* **Simple models**: If you want to apply a simple binary classifier (like Logistic Regression) to multiple classes, OvR is a straightforward extension.\n",
        "* **Independent classes**: If your classes are relatively **independent** of each other, OvR may be a good choice.\n",
        "* **Large number of classes**: OvR can be more computationally efficient when you have many classes, as you are training one classifier per class rather than a single, large, multi-class model.\n",
        "* **Faster training**: Since each binary classifier is trained separately, the training process can be more parallelizable, which may make OvR faster to train for large datasets.\n",
        "\n",
        "#### **Disadvantages of OvR:**\n",
        "\n",
        "* **Class imbalance**: OvR classifiers may perform poorly if there is significant **imbalance between classes**, as each classifier treats the other classes as a single combined negative class.\n",
        "* **Multiple models**: OvR requires training multiple classifiers (one for each class), which can lead to an increase in **computational cost** if the number of classes is very large.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Softmax Regression**\n",
        "\n",
        "**Softmax Regression** (also called **Multinomial Logistic Regression**) is an extension of logistic regression that directly handles multiclass classification by predicting probabilities for each class simultaneously. It uses the **Softmax function** to output a probability distribution across all classes.\n",
        "\n",
        "#### **When to use Softmax:**\n",
        "\n",
        "* **When classes are mutually exclusive**: If your classes are **mutually exclusive** (each sample belongs to only one class), Softmax is a natural fit, as it outputs a probability distribution that sums to 1, making it easier to interpret.\n",
        "* **Better class separation**: Softmax generally provides a better **joint optimization** because it models the entire problem in one single step rather than training independent classifiers.\n",
        "* **Balanced classes**: If your dataset has **balanced classes**, Softmax is often preferred as it naturally learns relationships between classes.\n",
        "* **Smaller number of classes**: If the number of classes is small to moderate, Softmax is computationally feasible, as it requires only one model and produces a single set of class probabilities.\n",
        "\n",
        "#### **Disadvantages of Softmax:**\n",
        "\n",
        "* **Not parallelizable**: Unlike OvR, Softmax trains a single model for all classes, which may not be as parallelizable in some cases.\n",
        "* **Complexity for large number of classes**: Softmax can become computationally expensive when the number of classes is very large, as the Softmax function requires computing the exponentials for each class during both training and prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of When to Use Each Approach**\n",
        "\n",
        "| **Criteria**                 | **One-vs-Rest (OvR)**                                            | **Softmax**                                                   |\n",
        "| ---------------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------- |\n",
        "| **Problem Type**             | Independent classes, binary classifiers per class                | Mutually exclusive classes, joint optimization                |\n",
        "| **Number of Classes**        | Works well with large number of classes                          | Suitable for small to moderate number of classes              |\n",
        "| **Class Imbalance**          | Can be sensitive to imbalance in each binary classifier          | Handles imbalanced classes better                             |\n",
        "| **Computational Efficiency** | Can be parallelized, computationally faster in some cases        | Requires more computation, less parallelizable                |\n",
        "| **Model Complexity**         | Simple to implement, requires multiple models                    | More complex, single model                                    |\n",
        "| **Training Time**            | May be faster, as each classifier is trained independently       | May take longer for large datasets                            |\n",
        "| **Prediction**               | Outputs class with highest probability from multiple classifiers | Outputs class with highest overall probability from one model |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways:**\n",
        "\n",
        "* **Use One-vs-Rest (OvR)** if you have a **large number of classes** and prefer simpler, independent binary classifiers. Itâ€™s also a good choice if you have **limited computational resources** and need parallelization.\n",
        "\n",
        "* **Use Softmax** if you want a more **jointly optimized** model that directly handles multiclass classification. It's ideal for **smaller class sets**, when **class relationships** matter, and when the dataset is **balanced**.\n",
        "\n",
        "In general, **Softmax** is preferred when dealing with **multiclass classification** in a single model, especially when classes are closely related and mutually exclusive. However, **One-vs-Rest** can be a practical choice for scalability and simplicity when you have a large number of classes or when you are using simpler models like Logistic Regression.\n",
        "\n",
        "\n",
        "20- How do we interpret coefficients in Logistic Regression ?\n",
        "\n",
        "Ans- ### **Interpreting Coefficients in Logistic Regression**\n",
        "\n",
        "In **Logistic Regression**, the coefficients (also called **weights**) represent the relationship between each feature (independent variable) and the log-odds of the outcome (dependent variable). Here's how to interpret these coefficients:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Logistic Regression Model Overview:**\n",
        "\n",
        "The logistic regression model can be written as:\n",
        "\n",
        "$$\n",
        "P(y=1|X) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $P(y=1|X)$ is the probability that the outcome $y = 1$ given the feature vector $X$.\n",
        "* $\\theta_0$ is the **intercept** (bias term).\n",
        "* $\\theta_1, \\theta_2, \\dots, \\theta_n$ are the **coefficients** (weights) associated with each feature $x_1, x_2, \\dots, x_n$.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Coefficients in Logistic Regression (Odds Ratio Interpretation)**\n",
        "\n",
        "The **coefficients** in logistic regression (denoted as $\\theta_1, \\theta_2, \\dots$) represent how each feature affects the **log-odds** of the dependent variable. The log-odds are the natural logarithm of the odds that the event occurs.\n",
        "\n",
        "#### **Log-Odds and Odds:**\n",
        "\n",
        "* The log-odds of the event happening is given by:\n",
        "\n",
        "  $$\n",
        "  \\text{log-odds} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
        "  $$\n",
        "* The **odds** are the exponentiation of the log-odds:\n",
        "\n",
        "  $$\n",
        "  \\text{odds} = e^{\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n}\n",
        "  $$\n",
        "\n",
        "  The **odds ratio** (OR) is the factor by which the odds change when a feature changes by 1 unit.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Interpreting Coefficients:**\n",
        "\n",
        "* **Positive Coefficients ($\\theta_i > 0$)**:\n",
        "\n",
        "  * If a coefficient is positive, an increase in that feature $x_i$ leads to an increase in the odds of the event occurring (i.e., $y = 1$).\n",
        "  * **Interpretation in terms of odds ratio**:\n",
        "    For each 1-unit increase in $x_i$, the odds of the event occurring are multiplied by $e^{\\theta_i}$.\n",
        "\n",
        "    Example:\n",
        "    If $\\theta_1 = 0.5$, the odds ratio is $e^{0.5} \\approx 1.65$, meaning that a 1-unit increase in $x_1$ increases the odds of the event happening by a factor of 1.65.\n",
        "\n",
        "* **Negative Coefficients ($\\theta_i < 0$)**:\n",
        "\n",
        "  * If a coefficient is negative, an increase in that feature $x_i$ leads to a decrease in the odds of the event occurring (i.e., $y = 1$).\n",
        "  * **Interpretation in terms of odds ratio**:\n",
        "    For each 1-unit increase in $x_i$, the odds of the event occurring are multiplied by $e^{\\theta_i}$.\n",
        "\n",
        "    Example:\n",
        "    If $\\theta_1 = -0.5$, the odds ratio is $e^{-0.5} \\approx 0.61$, meaning that a 1-unit increase in $x_1$ decreases the odds of the event happening by a factor of 0.61.\n",
        "\n",
        "* **Zero Coefficients ($\\theta_i = 0$)**:\n",
        "\n",
        "  * A coefficient of zero means that the corresponding feature has **no effect** on the odds of the event occurring.\n",
        "  * In other words, changing that feature does not alter the likelihood of the event.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Example Interpretation of Coefficients:**\n",
        "\n",
        "Suppose we have a logistic regression model predicting whether a person will buy a product ($y = 1$) based on two features: age ($x_1$) and income ($x_2$).\n",
        "\n",
        "The logistic regression equation might look like:\n",
        "\n",
        "$$\n",
        "\\log\\left(\\frac{P(y=1)}{1-P(y=1)}\\right) = -3 + 0.05 \\cdot x_1 + 0.02 \\cdot x_2\n",
        "$$\n",
        "\n",
        "* **Intercept ($\\theta_0 = -3$)**:\n",
        "  The intercept tells us the log-odds of the outcome when both age ($x_1$) and income ($x_2$) are 0. This is not usually interpretable on its own, but it sets the baseline for the model.\n",
        "\n",
        "* **Coefficient of Age ($\\theta_1 = 0.05$)**:\n",
        "  A 1-year increase in age is associated with an increase in the log-odds of purchasing the product by 0.05.\n",
        "  In terms of the odds ratio, the odds of purchasing the product increase by a factor of $e^{0.05} \\approx 1.051$. So, for each additional year of age, the odds of purchasing the product increase by about 5.1%.\n",
        "\n",
        "* **Coefficient of Income ($\\theta_2 = 0.02$)**:\n",
        "  A 1-unit increase in income is associated with an increase in the log-odds of purchasing the product by 0.02.\n",
        "  In terms of the odds ratio, the odds of purchasing the product increase by a factor of $e^{0.02} \\approx 1.02$. So, for each 1-unit increase in income, the odds of purchasing the product increase by about 2%.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Odds Ratio (OR) Interpretation:**\n",
        "\n",
        "The **odds ratio (OR)** is a more intuitive way to interpret the coefficients. The odds ratio for a feature $x_i$ is given by $e^{\\theta_i}$. Here's what it means:\n",
        "\n",
        "* **OR > 1**: For each 1-unit increase in $x_i$, the odds of the event occurring increase.\n",
        "* **OR < 1**: For each 1-unit increase in $x_i$, the odds of the event occurring decrease.\n",
        "* **OR = 1**: The feature $x_i$ has no effect on the odds of the event.\n",
        "\n",
        "---\n",
        "\n",
        "### **In Summary:**\n",
        "\n",
        "* **Interpretation of Coefficients**: Coefficients in Logistic Regression represent the change in the log-odds of the outcome for a one-unit change in the corresponding feature.\n",
        "* **Positive Coefficient**: An increase in the feature increases the odds of the event happening.\n",
        "* **Negative Coefficient**: An increase in the feature decreases the odds of the event happening.\n",
        "* **Odds Ratio**: The exponential of the coefficient represents how the odds of the event change for a one-unit increase in the feature.\n",
        "\n",
        "By interpreting the coefficients in this way, you can understand how each feature affects the likelihood of the outcome in a Logistic Regression model.\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "72euYDw_7lR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL QUESTION AND ANSWERS"
      ],
      "metadata": {
        "id": "_HtdUsxNCKVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga8AOP8M7Q-3",
        "outputId": "98a1ad5f-ed49-4a2e-d918-83dcb725d886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# 1- Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy ?\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2- Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Model Accuracy with L1 Regularization: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCcER63VCiEK",
        "outputId": "a0ff827c-2555-44d6-f803-25f433566cba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3- Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Model Accuracy with L2 Regularization (Ridge): {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients (weights for each feature)\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeiU339ZDHJB",
        "outputId": "1072b709-8c35-4070-ee23-3b52678dcf2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization (Ridge): 100.00%\n",
            "Model Coefficients:\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4- Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet') ?\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with Elastic Net Regularization\n",
        "# Here, l1_ratio=0.5 means we are using an equal mix of L1 and L2 regularization.\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Model Accuracy with Elastic Net Regularization: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients (weights for each feature)\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCwVayuvDXGh",
        "outputId": "6ad89918-4b32-4212-bea2-33fa21d539bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 100.00%\n",
            "Model Coefficients:\n",
            "[[ 0.38754434  1.77421091 -2.42266685 -0.70395535]\n",
            " [ 0.0771272   0.          0.         -0.5816593 ]\n",
            " [-1.25800089 -1.52766651  2.5971962   2.07741793]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5- Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (3 classes: 0, 1, 2)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with multi_class='ovr' (One-vs-Rest)\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Model Accuracy with One-vs-Rest (ovr) strategy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients (weights for each feature for each class)\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDZVi1y7DhMR",
        "outputId": "ea6fb242-c366-49b7-994e-ea47c479b8e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest (ovr) strategy: 100.00%\n",
            "Model Coefficients:\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6- Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2']         # Regularization type\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters found by GridSearchCV\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with Best Hyperparameters: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EocR0wezEPF4",
        "outputId": "0e7904d0-fd9b-4f32-9904-f903d5bfc487"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 1, 'penalty': 'l2'}\n",
            "Model Accuracy with Best Hyperparameters: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.85833333        nan 0.93333333        nan 0.96666667\n",
            "        nan 0.94166667        nan 0.95      ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7- Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Initialize StratifiedKFold with 5 splits\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Use cross_val_score to evaluate the model with Stratified K-Fold Cross-Validation\n",
        "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "print(f\"Accuracy for each fold: {scores}\")\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "average_accuracy = np.mean(scores)\n",
        "print(f\"Average Accuracy: {average_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDIDuDNHEekH",
        "outputId": "ae0b8637-36bd-4a64-8476-1364998dd1d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [1.         0.96666667 0.93333333 1.         0.93333333]\n",
            "Average Accuracy: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8- Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert the dataset into a pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target  # Add the target column to the DataFrame\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Features (all columns except 'target')\n",
        "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "# Target variable (the 'target' column)\n",
        "y = df['target']  # Target variable (last column)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAEuIqK5EuXt",
        "outputId": "24915573-51b9-444e-a5b3-d93d628d6183"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9- Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'C': uniform(0.01, 100),  # Regularization strength (log scale)\n",
        "    'penalty': ['l1', 'l2'],  # Regularization type\n",
        "    'solver': ['liblinear', 'saga']  # Solvers for Logistic Regression\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions,\n",
        "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Fit the model using RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters found by RandomizedSearchCV\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with Best Hyperparameters: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2o7E3iaE77b",
        "outputId": "90202600-7b24-409d-ee7e-79c86b3a9449"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': np.float64(2.0684494295802445), 'penalty': 'l2', 'solver': 'saga'}\n",
            "Model Accuracy with Best Hyperparameters: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10- Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Wrap the Logistic Regression model with OneVsOneClassifier\n",
        "ovo_model = OneVsOneClassifier(log_reg)\n",
        "\n",
        "# Train the model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy (One-vs-One): {accuracy * 100:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1Phk25nFub_",
        "outputId": "b1d78f1f-51df-43d5-d378-a1f688f1cc73"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (One-vs-One): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11- Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert the target variable to binary: Class 0 (Setosa) vs. Class 1/2 (Not Setosa)\n",
        "X = iris.data\n",
        "y = (iris.target == 0).astype(int)  # 1 if Setosa (class 0), else 0\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Setosa', 'Setosa'], yticklabels=['Not Setosa', 'Setosa'])\n",
        "plt.title('Confusion Matrix for Logistic Regression (Binary Classification)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Jol7yEBeGBSM",
        "outputId": "4a0aaca9-4b62-49e6-c6b9-f2eb71c34b85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12- Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = (iris.target == 0).astype(int)  # 1 if Setosa (class 0), else 0 (Not Setosa)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Calculate Precision\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Calculate Recall\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# Calculate F1-Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f'F1-Score: {f1:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWtoFCAVGQi8",
        "outputId": "a9869566-5051-4998-ba79-85f065e9fa8f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13- Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Create an imbalanced target variable\n",
        "X = iris.data  # Features\n",
        "y = (iris.target == 0).astype(int)  # Make it a binary classification (Setosa vs Not Setosa)\n",
        "\n",
        "# Introduce class imbalance by keeping only 10% of the minority class (Not Setosa)\n",
        "# Shuffling the dataset and then reducing the size of the majority class\n",
        "from collections import Counter\n",
        "\n",
        "# Check original class distribution\n",
        "print(f\"Original class distribution: {Counter(y)}\")\n",
        "\n",
        "# Create an imbalanced dataset by reducing the majority class\n",
        "X_imbalanced = X[y == 0][:10]  # Keep only 10 samples of class 0 (Setosa)\n",
        "y_imbalanced = y[y == 0][:10]\n",
        "\n",
        "X_imbalanced = np.concatenate((X_imbalanced, X[y == 1]))  # Add all samples of class 1 (Not Setosa)\n",
        "y_imbalanced = np.concatenate((y_imbalanced, y[y == 1]))\n",
        "\n",
        "# Check the new class distribution\n",
        "print(f\"New class distribution: {Counter(y_imbalanced)}\")\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report (Precision, Recall, F1-Score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD1g0MyMGfZQ",
        "outputId": "86ea259f-e746-4cec-857f-95fcf7ab3a8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution: Counter({np.int64(0): 100, np.int64(1): 50})\n",
            "New class distribution: Counter({np.int64(1): 50, np.int64(0): 10})\n",
            "Accuracy: 100.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "           1       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        12\n",
            "   macro avg       1.00      1.00      1.00        12\n",
            "weighted avg       1.00      1.00      1.00        12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14- Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset (using the Titanic dataset from a URL)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "titanic_data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(titanic_data.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# Fill missing Age values with the median age\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing Embarked values with the mode (most frequent value)\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop the 'Cabin' column as it has too many missing values\n",
        "titanic_data.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical variables into numerical variables\n",
        "# Convert 'Sex' column to numerical values (0 = male, 1 = female)\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_data['Sex'] = label_encoder.fit_transform(titanic_data['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numerical values (using one-hot encoding)\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Feature selection\n",
        "# Features (X) and target variable (y)\n",
        "X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Step 6: Train the Logistic Regression model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report for more details (Precision, Recall, F1-Score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P9eEg5EGww0",
        "outputId": "5de1bb04-0198-490d-9d04-5715757aeb94"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-4879dc302ad0>:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
            "<ipython-input-15-4879dc302ad0>:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.01%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15- Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "titanic_data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(titanic_data.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# Fill missing Age values with the median age\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing Embarked values with the mode (most frequent value)\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop the 'Cabin' column as it has too many missing values\n",
        "titanic_data.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical variables into numerical variables\n",
        "# Convert 'Sex' column to numerical values (0 = male, 1 = female)\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_data['Sex'] = label_encoder.fit_transform(titanic_data['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numerical values (using one-hot encoding)\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Feature selection\n",
        "# Features (X) and target variable (y)\n",
        "X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set (without scaling)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "\n",
        "# Evaluate the model without scaling\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling * 100:.2f}%\")\n",
        "\n",
        "# Print classification report for the model without scaling\n",
        "print(\"\\nClassification Report (Without Scaling):\")\n",
        "print(classification_report(y_test, y_pred_no_scaling))\n",
        "\n",
        "# Step 6: Apply Feature Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform the features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data with the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train the Logistic Regression model with scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set (with scaling)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model with scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f\"\\nAccuracy with Scaling: {accuracy_with_scaling * 100:.2f}%\")\n",
        "\n",
        "# Print classification report for the model with scaling\n",
        "print(\"\\nClassification Report (With Scaling):\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX0EFeHAG_NU",
        "outputId": "7442a958-6604-4c7f-9ca1-c40d90080a57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "Accuracy without Scaling: 81.01%\n",
            "\n",
            "Classification Report (Without Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n",
            "\n",
            "Accuracy with Scaling: 81.01%\n",
            "\n",
            "Classification Report (With Scaling):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1d8fd2ab1c71>:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
            "<ipython-input-16-1d8fd2ab1c71>:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16- Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "titanic_data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(titanic_data.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# Fill missing Age values with the median\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing Embarked values with the mode\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop the 'Cabin' column as it has too many missing values\n",
        "titanic_data.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical variables into numerical variables\n",
        "# Convert 'Sex' column to numerical values (0 = male, 1 = female)\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_data['Sex'] = label_encoder.fit_transform(titanic_data['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numerical values (using one-hot encoding)\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Feature selection\n",
        "# Features (X) and target variable (y)\n",
        "X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict the probabilities on the test set\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Step 7: Compute the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f'ROC-AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Step 8: Plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc_value = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc_value:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (no skill)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XkjTVjeAHUVc",
        "outputId": "44f5f96e-7953-4adc-c514-0f2b082bd4b2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "ROC-AUC Score: 0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-c09bf382b4c8>:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
            "<ipython-input-17-c09bf382b4c8>:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17- Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "titanic_data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(titanic_data.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# Fill missing Age values with the median\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing Embarked values with the mode\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop the 'Cabin' column as it has too many missing values\n",
        "titanic_data.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical variables into numerical variables\n",
        "# Convert 'Sex' column to numerical values (0 = male, 1 = female)\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_data['Sex'] = label_encoder.fit_transform(titanic_data['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numerical values (using one-hot encoding)\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Feature selection\n",
        "# Features (X) and target variable (y)\n",
        "X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model using accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy with C=0.5: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Step 8: Print classification report for additional metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4lSCZxaHpdJ",
        "outputId": "6bf327d6-d420-4bca-f723-78520432f8d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-19363ca5a4fd>:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
            "<ipython-input-18-19363ca5a4fd>:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 80.45%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       105\n",
            "           1       0.78      0.73      0.76        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18- Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "titanic_data = pd.read_csv(url)\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "titanic_data.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical variables into numerical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_data['Sex'] = label_encoder.fit_transform(titanic_data['Sex'])\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Feature selection\n",
        "X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Obtain the coefficients\n",
        "coefficients = model.coef_[0]\n",
        "features = X.columns\n",
        "\n",
        "# Step 7: Display features and their corresponding coefficients\n",
        "coeff_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})\n",
        "coeff_df['Absolute Coefficient'] = np.abs(coeff_df['Coefficient'])\n",
        "\n",
        "# Sort the coefficients by their absolute value to identify important features\n",
        "sorted_coeff_df = coeff_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Step 8: Print the sorted features based on their importance (absolute coefficient values)\n",
        "print(\"Important Features based on Model Coefficients:\")\n",
        "print(sorted_coeff_df)\n",
        "\n",
        "# Step 9: Evaluate the model (optional)\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXtpLE1UIOTC",
        "outputId": "d9ae0371-34e5-446a-c8a1-13b7ed248cf2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features based on Model Coefficients:\n",
            "      Feature  Coefficient  Absolute Coefficient\n",
            "1         Sex    -2.590612              2.590612\n",
            "0      Pclass    -0.938550              0.938550\n",
            "7  Embarked_S    -0.401270              0.401270\n",
            "3       SibSp    -0.294975              0.294975\n",
            "6  Embarked_Q    -0.112181              0.112181\n",
            "4       Parch    -0.108025              0.108025\n",
            "2         Age    -0.030592              0.030592\n",
            "5        Fare     0.002570              0.002570\n",
            "\n",
            "Model Accuracy: 81.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-719743254e76>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
            "<ipython-input-19-719743254e76>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19- Write a Python program to train Logistic Regression and evaluate its performance using Cohenâ€™s Kappa Score\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "titanic_data = pd.read_csv(url)\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "titanic_data.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical variables into numerical variables\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_data['Sex'] = label_encoder.fit_transform(titanic_data['Sex'])\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Feature selection\n",
        "X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Compute Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwMz4shjIgzd",
        "outputId": "24a2b1db-3040-43d9-f42a-fd237ecec325"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-44f4c332f75d>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
            "<ipython-input-20-44f4c332f75d>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20- Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification:\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])  # 0 = male, 1 = female\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 4: Select features and target\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 5: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Predict probabilities\n",
        "y_scores = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "# Step 8: Compute precision, recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Step 9: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "HfQudsARI0ZQ",
        "outputId": "98e63cfa-fc85-42cf-fa25-19c5ca826b96"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-fd9fd6492d6e>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
            "<ipython-input-21-fd9fd6492d6e>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21- Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])  # 0 = male, 1 = female\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 4: Feature selection\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 5: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train and compare models with different solvers\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=200)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        results[solver] = acc\n",
        "    except Exception as e:\n",
        "        results[solver] = f'Failed: {e}'\n",
        "\n",
        "# Step 7: Display results\n",
        "print(\"Accuracy comparison by solver:\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"Solver: {solver.ljust(10)} --> Accuracy: {acc if isinstance(acc, float) else acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV8ZIKMrJNy-",
        "outputId": "e72108cd-3976-4a44-c61d-5da31f1b6d02"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-0b54b9ca1d48>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
            "<ipython-input-22-0b54b9ca1d48>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy comparison by solver:\n",
            "Solver: liblinear  --> Accuracy: 0.7821229050279329\n",
            "Solver: saga       --> Accuracy: 0.6703910614525139\n",
            "Solver: lbfgs      --> Accuracy: 0.8100558659217877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22- Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])  # 0 = male, 1 = female\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 4: Feature selection\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 5: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Predict and evaluate using MCC\n",
        "y_pred = model.predict(X_test)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Step 8: Output\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soIOcs0lJiAe",
        "outputId": "1f96dcc7-632d-4182-e16f-d1c8d3a8c0c3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-c1bf38f5f066>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
            "<ipython-input-23-c1bf38f5f066>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23- Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])  # 0 = male, 1 = female\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 4: Feature selection\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 5: Split the data\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --------- Model on Raw Data ----------\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train_raw, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test_raw)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# --------- Model on Standardized Data ----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_test_scaled = scaler.transform(X_test_raw)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# --------- Comparison Result ----------\n",
        "print(f\"Accuracy on Raw Data         : {acc_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {acc_scaled:.4f}\")\n",
        "\n",
        "if acc_scaled > acc_raw:\n",
        "    print(\"âœ… Feature scaling improved model performance.\")\n",
        "elif acc_scaled < acc_raw:\n",
        "    print(\"âš ï¸ Feature scaling reduced model performance.\")\n",
        "else:\n",
        "    print(\"â„¹ï¸ Feature scaling had no impact on accuracy.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aoa4Z4XVKMbD",
        "outputId": "fe8bf314-9163-48aa-dd14-e0944b5c41b4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data         : 0.8101\n",
            "Accuracy on Standardized Data: 0.8101\n",
            "â„¹ï¸ Feature scaling had no impact on accuracy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-637c1bf65892>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
            "<ipython-input-24-637c1bf65892>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24- Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])  # 0 = male, 1 = female\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 4: Define features and target\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 5: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 6: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Set up parameter grid for C\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'],  # L2 is the default; L1 would require 'liblinear' or 'saga'\n",
        "    'solver': ['lbfgs'],  # Supports L2 and multiclass\n",
        "}\n",
        "\n",
        "# Step 8: Perform GridSearchCV\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Best model and evaluation\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 10: Output results\n",
        "print(f\"Best C value found: {grid.best_params_['C']}\")\n",
        "print(f\"Test Set Accuracy with best C: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbhKOWKtKlWy",
        "outputId": "0a0f6829-cca5-4e5c-a5bd-69e322de3586"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-471e370ca918>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
            "<ipython-input-25-471e370ca918>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value found: 0.01\n",
            "Test Set Accuracy with best C: 0.7933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25- Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Data preprocessing\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])  # 0 = male, 1 = female\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Feature selection\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 4: Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 5: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Save the model and scaler using joblib\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "print(\"âœ… Model and scaler saved.\")\n",
        "\n",
        "# Step 8: Load the model and scaler\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Step 9: Predict using the loaded model\n",
        "sample_data = X_test[:5]  # Take 5 samples\n",
        "y_true = y_test[:5]\n",
        "\n",
        "y_pred = loaded_model.predict(sample_data)\n",
        "\n",
        "# Step 10: Output\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"True Labels   :\", list(y_true.values))\n",
        "print(\"Predicted     :\", list(y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UECUo48lK8jg",
        "outputId": "2ad4c11e-3bca-414b-a21a-dc20b729c97a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model and scaler saved.\n",
            "\n",
            "Sample Predictions:\n",
            "True Labels   : [np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
            "Predicted     : [np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-5b4ff7d1963e>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
            "<ipython-input-26-5b4ff7d1963e>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ld7V2hlbLTMZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}